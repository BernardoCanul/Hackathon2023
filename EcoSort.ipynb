{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **EcoSort**\n",
        "EcoSort es un proyecto open source creado durante el Hackathon 2023 como una herramienta que podría ayudar a combatir el problema de la contaminación. El principal objetivo de EcoSort es el poder ayudar a la población en general en la correcta segmentación de sus desperdicios.\n",
        "\n",
        "Para realizar esta tarea se optó por entrenar una red neuronal en Tensorflow que tenga como entrada la imagen del objeto y a su salida la clasificación en donde debe ser colocada.\n",
        "\n",
        "*Uno de los objetivos del proyecto es que la efectividad del modelo sea mayor a un 90%*"
      ],
      "metadata": {
        "id": "K26ANsFypOXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de librerías necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "hawRKwV9q7AX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "tn8Z1LK3tKxJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtención de datos de entrenamiento\n",
        "\n",
        "Para el entrenamiento de este modelo se ha encontrado el dataset [\"Garbage Classification](https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification).\n",
        "\n",
        "Una vez obtenidos los datos se realizará el preprocesamiento de los mismos, lo cual consistirá de normalizar las imagen y reescalarlas todas a un tamaño de (224,224); Además de esto se aplicará la técnica de \"Data Augmentation\" para poder generar mayor cantidad de datos de entrenamiento para la red."
      ],
      "metadata": {
        "id": "9eGt7v09qZGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unrar\n",
        "!unrar x \"test.rar\"\n",
        "!unrar x \"train.rar\""
      ],
      "metadata": {
        "id": "Qk8FDMsJ7zg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preproces the data, normalize it and apply data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range = 20,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range = 20,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2)"
      ],
      "metadata": {
        "id": "uQ_duhMDua5W"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup dir of the data\n",
        "train_dir = \"/content/train\"\n",
        "test_dir = \"/content/test\"\n",
        "\n",
        "# Import data from directories and batch it\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224,224),\n",
        "                                               class_mode=\"categorical\",\n",
        "                                               seed=42)\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             batch_size=32,\n",
        "                                             target_size=(224,224),\n",
        "                                             class_mode=\"categorical\",\n",
        "                                             seed=42)\n"
      ],
      "metadata": {
        "id": "lTnOhoVmxPwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fae0744-9d50-4102-b3f3-58994f2bb0e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2025 images belonging to 6 classes.\n",
            "Found 502 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación del modelo"
      ],
      "metadata": {
        "id": "mSJCUICxqVgB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wk3jqhUWoZX9"
      },
      "outputs": [],
      "source": [
        "# Importar las capas que se utilizarán\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación del primer modelo utilizando transfer-learning \n",
        "model_url= \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
        "\n",
        "feature_extraction_layer = hub.KerasLayer(model_url,\n",
        "                                          trainable=False,\n",
        "                                          name=\"feature_extraction_layer\",\n",
        "                                          input_shape=(224,224,3))\n",
        "\n",
        "model_1 = Sequential([\n",
        "    feature_extraction_layer,\n",
        "    Dense(6, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model_1.compile(loss=\"categorical_crossentropy\",\n",
        "                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "history_1 = model_1.fit(train_data,\n",
        "                         epochs=20,\n",
        "                         steps_per_epoch=len(train_data),\n",
        "                         validation_data=test_data,\n",
        "                         validation_steps=len(test_data),\n",
        "                        callbacks=[early_stopping_callback])"
      ],
      "metadata": {
        "id": "YhAhwHl8y8Ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e163f5-27a1-4a8f-9358-65d76cc7b4ce"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "64/64 [==============================] - 48s 603ms/step - loss: 0.5952 - accuracy: 0.7911 - val_loss: 0.7665 - val_accuracy: 0.8028\n",
            "Epoch 2/20\n",
            "64/64 [==============================] - 37s 584ms/step - loss: 0.2844 - accuracy: 0.8978 - val_loss: 0.7780 - val_accuracy: 0.8048\n",
            "Epoch 3/20\n",
            "64/64 [==============================] - 37s 579ms/step - loss: 0.2278 - accuracy: 0.9121 - val_loss: 0.7509 - val_accuracy: 0.7948\n",
            "Epoch 4/20\n",
            "64/64 [==============================] - 37s 587ms/step - loss: 0.1787 - accuracy: 0.9314 - val_loss: 0.7623 - val_accuracy: 0.8267\n",
            "Epoch 5/20\n",
            "64/64 [==============================] - 37s 584ms/step - loss: 0.1468 - accuracy: 0.9467 - val_loss: 0.7051 - val_accuracy: 0.8267\n",
            "Epoch 6/20\n",
            "64/64 [==============================] - 37s 579ms/step - loss: 0.1155 - accuracy: 0.9625 - val_loss: 0.8070 - val_accuracy: 0.8327\n",
            "Epoch 7/20\n",
            "64/64 [==============================] - 37s 583ms/step - loss: 0.0966 - accuracy: 0.9689 - val_loss: 0.7629 - val_accuracy: 0.8147\n",
            "Epoch 8/20\n",
            "64/64 [==============================] - 37s 583ms/step - loss: 0.1008 - accuracy: 0.9649 - val_loss: 0.7492 - val_accuracy: 0.8247\n",
            "Epoch 9/20\n",
            "64/64 [==============================] - 37s 586ms/step - loss: 0.0945 - accuracy: 0.9674 - val_loss: 0.9338 - val_accuracy: 0.8267\n",
            "Epoch 10/20\n",
            "64/64 [==============================] - 37s 582ms/step - loss: 0.0861 - accuracy: 0.9733 - val_loss: 0.8036 - val_accuracy: 0.8227\n",
            "Epoch 11/20\n",
            "64/64 [==============================] - 38s 588ms/step - loss: 0.0837 - accuracy: 0.9709 - val_loss: 0.7993 - val_accuracy: 0.8426\n",
            "Epoch 12/20\n",
            "64/64 [==============================] - 37s 585ms/step - loss: 0.0902 - accuracy: 0.9684 - val_loss: 1.0062 - val_accuracy: 0.8108\n",
            "Epoch 13/20\n",
            "64/64 [==============================] - 37s 578ms/step - loss: 0.0881 - accuracy: 0.9669 - val_loss: 0.8634 - val_accuracy: 0.8386\n",
            "Epoch 14/20\n",
            "64/64 [==============================] - 37s 581ms/step - loss: 0.0724 - accuracy: 0.9788 - val_loss: 0.9289 - val_accuracy: 0.8367\n",
            "Epoch 15/20\n",
            "64/64 [==============================] - 37s 586ms/step - loss: 0.0608 - accuracy: 0.9773 - val_loss: 1.0396 - val_accuracy: 0.8267\n",
            "Epoch 16/20\n",
            "64/64 [==============================] - 37s 581ms/step - loss: 0.0522 - accuracy: 0.9827 - val_loss: 0.9367 - val_accuracy: 0.8526\n",
            "Epoch 17/20\n",
            "64/64 [==============================] - 37s 577ms/step - loss: 0.0560 - accuracy: 0.9798 - val_loss: 0.9311 - val_accuracy: 0.8367\n",
            "Epoch 18/20\n",
            "64/64 [==============================] - 37s 578ms/step - loss: 0.0671 - accuracy: 0.9773 - val_loss: 1.0998 - val_accuracy: 0.8108\n",
            "Epoch 19/20\n",
            "64/64 [==============================] - 37s 581ms/step - loss: 0.0873 - accuracy: 0.9684 - val_loss: 1.0089 - val_accuracy: 0.8267\n",
            "Epoch 20/20\n",
            "64/64 [==============================] - 37s 579ms/step - loss: 0.0771 - accuracy: 0.9719 - val_loss: 1.0431 - val_accuracy: 0.8367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.save(\"modelo_85.h5\")"
      ],
      "metadata": {
        "id": "R9y2QkOSKVbq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.save_weights(\"Weights/model_85_weights.cpkt\")"
      ],
      "metadata": {
        "id": "9_B_qbkSKmLa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "hREpEiZTOsYC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(\"/content/test/paper/paper534.jpg\")\n",
        "img_res = cv2.resize(img, (224,224))\n",
        "img_res = img_res/255.\n",
        "\n",
        "prediction = model_1.predict(tf.expand_dims(img_res, axis=0))\n",
        "prediction = tf.squeeze(prediction)\n",
        "\n",
        "max_index = tf.argmax(prediction)\n",
        "max_index.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0maOdga5O7ja",
        "outputId": "2a8b37c3-8c96-48c3-dedb-fd6594c3387e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_image(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img_res = cv2.resize(img, (224,224))\n",
        "  img_res = img_res/255.0\n",
        "\n",
        "  return tf.expand_dims(img_res, axis=0)"
      ],
      "metadata": {
        "id": "rJF9G7FlRi-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "modelo2 = keras.models.load_model(\"/content/modelo_85.h5\",\n",
        "                                  custom_objects={'KerasLayer':hub.KerasLayer})"
      ],
      "metadata": {
        "id": "KKhqWc40WeuL"
      },
      "execution_count": 64,
      "outputs": []
    }
  ]
}